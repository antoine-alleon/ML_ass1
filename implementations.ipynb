{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#6 basic method implementations as described above in step 2: We want you to implement and use the methods \n",
    "#we have seen in class and in the labs. You will need to provide working implementations of the functions in Table 1. \n",
    "#If you have not finished them during the labs, you should start by implementing the first ones to have a working \n",
    "#toolbox before diving in the dataset.\n",
    "\n",
    "#Return type: Note that all functions should return: (w, loss), which is the last weight vector of the method, \n",
    "#and the corresponding loss value (cost function). Note that while in previous labs you might have kept track of \n",
    "#all encountered w for iterative methods, here we only want the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_MSE(y, tx, w):\n",
    "    MSE=((np.linalg.norm(y-(np.dot(tx,w))))**2)/(2*len(y))\n",
    "    \n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx, n_max):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    xx=np.delete(tx,0,1) #save x vector\n",
    "    for i in range(n_max-2):\n",
    "        np.concatenate(tx,np.power(xx,i+2)) #add powers of x to tx\n",
    "    w_opt=np.linalg.solve(np.matmul(np.transpose(tx),tx),np.dot(np.transpose(tx),y)) #compute weights\n",
    "    mse=compute_MSE(y,tx,w_opt) #compute error\n",
    "    \n",
    "    return mse, w_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, n_max, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    lambda_=lambda_/(2*np.shape(y))\n",
    "    xx=np.delete(tx,0,1) #save x vector\n",
    "    for i in range(n_max-2):\n",
    "        np.append(tx,np.power(xx,i+2)) #add powers of x to tx\n",
    "    w_opt=np.linalg.solve(np.matmul(np.transpose(tx),tx)+lambda_*np.identity(np.shape(y)),np.dot(np.transpose(tx),y)) #compute weights\n",
    "    mse=compute_MSE(y,tx,w_opt) #compute error\n",
    "    \n",
    "    return mse,w_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t): #So we want to return a value between 0 and 1 to make sure we are actually representing a probability. To do this we will make use of the logistic function.\n",
    "    return 1.0 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):  # the goal is to compute the cost with the technique of log likelohood (negative)\n",
    "    fontion = sigmoid(np.dot(tx,w))\n",
    "    y_T=y.T\n",
    "    loss = np.dot(y_T, np.log(fonction)) + np.dot((1 - y).T, np.log(1 - fonc))\n",
    "    return np.squeeze(- loss)  #squeeze Remove single-dimensional entries from the shape of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w): # the goal is to calculate the gradient of losses\n",
    "    fonction = sigmoid(dot(tx,w))\n",
    "    gradient = dot(tx.T, fonction - y)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma): #the goal is to compute of step of the gradient descent method using logistic regression\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    w = w-gamma*gradient\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w): #the goal is to calculate the loss and the gradient thanks to fonction above\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_logistic_regression(y, tx, w, lambda_): \n",
    "    samples = y.shape[0]\n",
    "    loss = calculate_loss(y, tx, w) + lambda_ * np.squeeze(dot(w.T,w))\n",
    "    gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_regularized_gradient(y, tx, w, gamma, lambda_):\n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w -= gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # ***************************************************\n",
    "    # polynomial basis function:\n",
    "    phi_mat = np.empty([len(x),degree+1])\n",
    "    for i in range(len(x)):\n",
    "        for j in range(degree+1):\n",
    "            phi_mat[i,j] = x[i]**j\n",
    "                \n",
    "    return phi_mat\n",
    "    # this function should return the matrix formed\n",
    "    # by applying the polynomial basis to the input data\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    # ***************************************************\n",
    "    test_indice = k_indices[k]\n",
    "    train_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    train_indice = train_indice.reshape(-1)\n",
    "    y_test = y[test_indice]\n",
    "    y_train = y[train_indice]\n",
    "    x_test = x[test_indice]\n",
    "    x_train = x[train_indice]\n",
    "    \n",
    "    # form data with polynomial degree: TODO\n",
    "    # ***************************************************\n",
    "    tx_train = build_poly(x_train, degree)\n",
    "    tx_test = build_poly(x_test, degree)\n",
    "    \n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "    w = ridge_regression(y_train, tx_train, lambda_)\n",
    "    \n",
    "    # calculate the loss for train and test data: TODO\n",
    "    # ***************************************************\n",
    "    loss_train = np.sqrt(2 * compute_mse(y_train, tx_train, w))\n",
    "    loss_test = np.sqrt(2 * compute_mse(y_test, tx_test, w))\n",
    "   \n",
    "    return loss_train, loss_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
